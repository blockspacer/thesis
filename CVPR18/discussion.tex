% %!TEX root = main.tex

\section{Discussion}

In this paper, we present what we believe to be the first analysis of human sensitivity on estimation errors for pitch, roll and field of view in the context of virtual object insertion. To this end, we performed a large-scale user study on Mechanical Turk, which evaluates how reliably the participants were able to distinguish between two images with virtual objects composited with ground truth and distorted camera parameters. Our study reveals that humans are not always sensitive to large errors, especially when the roll is pronounced, or when the field of view is underestimated. We also present a CNN-based single image calibration estimation method which yields state-of-the-art performance, enabling applications such as image retrieval, geometrically-consistent 2D object transfer, and virtual 3D object insertion. Upon investigation, it was revealed that the learned model is looking for semantically meaningful vanishing lines, making parallels with geometrically-based auto-calibration techniques. Finally, we leverage the user study results to define a distance function based on human perception, which is used to compare our CNN to previous approaches.

Despite this progress, our approach still suffers from a few limitations. While the trained CNN works very robustly in a large number of realistic scenarios, extreme pitch angles (e.g. looking straight down) cannot be represented by the current horizon line parameterization. Furthermore, the perceptual distance function defined in sec.~\ref{sec:cnn-evaluation-perception} could be used as a loss function to train the neural network, putting emphasis on training images where humans are more sensitive to errors. Lastly, the robustness of our model could be coupled with the accuracy of a geometric-based method by taking advantage of the detected semantically meaningful vanishing lines.

