%!TEX root = main.tex
\chapter*{Introduction}         % ne pas num√©roter
\phantomsection\addcontentsline{toc}{chapter}{Introduction} % inclure dans TdM

% General
Natural images are but a glimpse of captured light. Whether they depict a morning in the park, an afternoon at the beach, an evening in the living room, they all resonate strongly with the human visual system. We are able to appreciate such images because evolution endowed us with high dynamic range wide-angle stereoscopic sensors, namely the eyes. But even equipped with those high-performance sensors, our complex nervous system evolved to heavily rely on prior experience to perform its task. In human brains, the main central connection to the optic nerve is the lateral geniculate nucleus (LGN), the relay center for the visual pathway. It has been shown that only 5--10\% of the input to the LGN derives from the retina, the remainder being connected to other regions of the brain~\cite{van2000relative}. Therefore, even though the brain has access to sensors with great characteristics, it performs most vision tasks by relying on past memories. This is why we are able to navigate through complex environments like university offices or correctly estimate distances, even with a single eye opened.
In short, the meaning of the perceived light intensity is not only defined by its surroundings and context, but also by what happened in the past, as shaped by decades of learning experienced by the observer. 

As we begin to better understand our own visual system, it becomes clear that, just like humans, high level computer vision tasks must also rely on prior knowledge to be performed successfully. In this setting, this knowledge must be gathered from large amount of image data. Fortunately, the advent of social media has brought a phenomenal influx of images of all sorts each day to public databases, enabling the development of data-hungry machine learning algorithms such as deep neural networks. Such data-driven approaches are not only possible due to the newly available datasets, but also given the recent increase in computing speed and storage capacity of contemporary computers, allowing to handle the staggering amount of data publicly available nowadays. These machine learning methods bring a new paradigm to tackle vision problems: learning priors on natural images. These priors (initial beliefs on a probability distribution) are effectively additional constraints that can complement classical physics- or geometry-based approaches, which can improve solutions to ill-posed problems. 

This is the main topic of this dissertation: learning priors through machine learning to understand and solve problems that are ill-posed when considering them exclusively from a physics or geometric perspective. Specifically, three problems will be covered: first, the problem of 3D surface reconstruction using photometric cues, followed by outdoor lighting estimation and finally geometric camera calibration. For all those scenarios, we are interested in cases when the environment is mostly uncontrolled, resulting in under-constrained problems, impossible to solve robustly with classical approaches. 

% First chapter
First, we will focus on recovering the surface geometry of a 3D object, which can be done by photometric stereo (PS), a popular dense shape reconstruction technique that has matured extensively over nearly 40 years~\cite{woodham-opteng-80}. % to work with complex materials and lighting conditions~\cite{alldrin-cvpr-08,basri-ijcv-07,johnson-cvpr-11,oxholm-eccv-12}.
%Given the excellent PS results obtained in carefully designed laboratory setups,
Simply put, this technique proposes to recover the surface normals of a 3D object observed under varying illumination from a single viewpoint. 
PS is reputed to give accurate surface normal estimations in fully calibrated environments, where lighting is controlled. 

Recent investigations have turned to the more challenging problem of applying PS in outdoor environments, under uncontrolled, natural illumination. To do so, people used the sun as a point light source to model outdoor lighting conditions. However, the sun follows a mostly coplanar path throughout a single day, leading to photometric cues which do not create sufficient constraints to robustly solve the PS problem. Recent approaches proposed to capture images over the course of many months~\cite{ackermann-cvpr-12,abrams-eccv-12}. This time interval provides enough shifting to the solar plane to constrain correctly the PS problem under the point light source assumption. Unfortunately, waiting for several months is tedious and impractical. 

In this thesis, we propose to leverage the richness of natural illumination to solve the outdoor photometric stereo problem in shorter time intervals. This brings us to our first main contribution: 

\begin{quotation}
\textbf{Short-Term Photometric Stereo} We present a systematic analysis of the expected performance of PS algorithms in outdoor settings on a single day or less, and propose to solve the short-term PS problem under various weather conditions by compensating the missing information from photometric cues with learned priors. 
\end{quotation}

By using a richer lighting model than the point light source to solve the outdoor PS problem, we are able to understand why and when outdoor photometric cues alone can result in a stable surface reconstruction. We further show that, in some cases, photometric cues alone cannot solve this problem robustly. In such cases, we propose to augment the photometric cues with learned priors in order to solve the PS problem in those hard cases. 

The second axis of research presented in this thesis explores the problem of estimating lighting from a single outdoor image. Outdoor lighting conditions throughout the day are mainly governed by the position of the sun and the sky turbidity (quantity of aerosols like water vapor present in the atmosphere). The goal is to estimate those parameters from an image of a generic scene taken with a standard camera. What makes this problem particularly challenging is the uncertainty present in generic images. Indeed, the sky and the sun may not be directly visible in the image, so we must estimate their properties by observing their impact on the scene. A classical way to do so would be to rely on known properties of the scene like its geometry and the reflectance of its surfaces and obtain the illumination conditions using some inverse rendering method. However, those scene properties are typically unknown and estimating them robustly is still an open research question. Relying on explicit scene properties estimations to estimate lighting conditions is a process prone to errors. 

In this dissertation, we argue that machine learning can be used to learn priors on generic scenes and natural illumination and overcome the problem of explicit scene parameter estimation. Using this additional information, it is possible to improve current single-image lighting estimation techniques, leading to our second contribution: 

\begin{quotation}
\textbf{Single Image Outdoor Lighting Estimation} We present a single-image learning-based approach to perform outdoor lighting estimation on generic scenes under natural daylight. 
\end{quotation}

Estimating the sun position from shading cues or the sky (when present in the image) is a hard task that requires the estimation of the scene geometry and the segmentation of the sky. While we have no formal proof that our method performs those tasks, our experiments and overall performance indicates that our method understands at least some basic photometric cues, even though we never explicitly provided it information about light physics. This implicit understanding of light allows our method to yield state-of-the-art illumination estimation performance, enabling photorealistic virtual object insertions and automatic relighting.

The last research axis focuses on geometric camera calibration from a single image of a generic scene. This task usually requires some specific object to be inserted in the scene, like a checkerboard pattern on a planar surface. Then, around a dozen images are taken, where this checkerboard is positioned in various locations and orientations in the image. Having to insert an object in the scene and capturing multiple images makes this process tedious. Furthermore, the large corpus of images that have already been taken typically does not contain such a specific object, making them inappropriate for classical geometric camera calibration. To simplify geometric calibration, one could perform it using the content present in a single image. However, this setup turns the calibration process into a severely ill-posed problem. 

In this thesis, we claim that it is possible to constrain the camera calibration problem to be robustly solved using a single image using learned priors on natural images, leading to our third contribution: 

\begin{quotation}
\textbf{Single Image Camera Calibration} We propose a single-image learning-based approach to perform geometric camera calibration. The proposed method works on generic scenes and do not require a specific object to be present in the image. 
\end{quotation}

Inferring geometric camera calibration directly from image pixels using a deep neural network improves performance and robustness over classical approaches. In particular, our method obtains state-of-the-art focal length and horizon estimation performance when comparing against other single image methods. However, we argue that we are often more interested in the human perception of accuracy instead of comparing against the ground truth. To this end, we performed a large-scale study of the human sensitivity to calibration errors and, based on this study, developed a novel perceptual measure to demonstrate that our deep calibration network outperforms other methods also in terms of human perception. 

Recent advances in machine learning allows learned models to encode the structure of data in large datasets, surpassing what would be possible with handcrafted feature extractors. However, this learned structure and priors are implicitly encoded inside the model, meaning that what has been learned is not directly available once the model is trained. Throughout the contributions proposed by this thesis, we strive to understand through indirect analysis the behavior of the learned model. Concretely, we perform experiments like ablation studies understand how the model perform when some information is missing and show low-dimensional embedding representations to map the inner space learned by our method. 

%Recent advances in machine learning allow for robust detection and classification of objects in images. By recognizing the objects present in a scene and comparing with their typical size in the physical world, one could estimate the focal length of the image. A similar process can be done with the object orientation and position, giving camera pitch and roll. Calibration would then be a matter of capturing common objects of the world and extracting their characteristics from the encoding of a machine learning model. 

The three contributions can have a direct impact on various applications. For instance, surface reconstruction can allow the preservation of historical landmarks or cultural heritage like statues and architectural features in high-definition. It is also useful for the entertainment industry, where scanning real-life models is critical for realism in video games or the movie industry. Single-image lighting and geometric camera calibration enables automated photorealistic virtual object insertion, image alteration and relighting. As such, some of the proposed methods have already been transfered to Dimension, the new 3D editing software from Adobe Systems. It could also be applied to forensics, allowing the analysis of potentially manipulated images. 

\section*{Overview}

This thesis is grouped into three main axes: 1) surface reconstruction through photometric cues, 2) learning-based lighting estimation and 3) single image camera calibration. All research axes are explored through a data-driven paradigm and exploit both existing datasets and data that we acquired. 
% Concretely, each project leverages between thousands and millions of images to train models by optimizing several millions of parameters, executing trillions of floating-point operations within seconds to process data yielding hundreds of thousands of dimensions. It would be a euphemism to say that the presented research could not have been performed without the recent advances in both computing and storage.

First, chapter~\ref{ch1} provides an in-depth analysis of the information contained in photometric cues throughout a single day and gives performance bounds on Photometric Stereo when performed on intervals down to a single hour. Upon investigation, we found that partially cloudy days brought enough constraints to solve the PS problem using an adapted PS algorithm. However, we show that sunny days in general are lacking the constraints to provide a stable surface reconstruction. In chapter~\ref{ch2}, we go one step further than what is possible using exclusively photometric cues. Since there is not enough information in a single sunny day to do a stable surface reconstruction solely from photometric cues, we employ a deep learning model to learn priors on local surface geometry and sun trajectory patterns. This additional information brings enough supplemental constraints to the PS problem to allow stable surface reconstructions. 

Chapters~\ref{ch3} and~\ref{ch4} propose to extract priors from large datasets and utilize them to improve single-image outdoor lighting and camera calibration, respectively. Most current outdoor lighting estimation techniques rely exclusively on handcrafted features, limiting their application to a determined environment, for example urban scenes~\cite{lalonde-ijcv-12}. We propose a lighting estimation approach that is robust to generic scenes. We are able to do so by learning features on a large number of scenes that captures the essence of natural illumination, while devoid of specific content. Finally, the last chapter covers the problem of camera calibration from a single image. An emphasis is put on focal length estimation and extrinsic calibration with respect to the earth. To do so, we focus on finding the horizon within the image, even when it is hidden, like in most indoor scenes. We further analyze human tolerance to errors on those calibration parameters. 



%\bibliographystyle{abbrvnat}
%\bibliography{library}

