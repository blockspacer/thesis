%!TEX root = main.tex
\chapter*{Introduction}         % ne pas num√©roter
\phantomsection\addcontentsline{toc}{chapter}{Introduction} % inclure dans TdM

% General
Natural images are but a glimpse of captured light. They depict a morning in the park, some afternoon in the living room or any other scene that we can witness, yet they all resonate strongly with the human visual system. We are able to appreciate such images because evolution endowed us with high dynamic range wide-angle stereoscopic sensors, namely the eyes. But even equipped with those high-performance sensors, our complex nervous system evolved to rely heavily on prior experience to perform its task. In fact, only 5-10\% of the input of the LGN---the relay center for the visual pathway in the mammal's brain---derives from the retina, the rest being connected to various regions of the brain. Humans thus perform vision tasks mainly from past memories. This is why we are able to navigate through complex environments like university offices or estimate correctly distances, even with a single eye. These are only fragments of why automating the human visual system through computer vision tasks is so difficult. While the brain has access to sensors far superior to the currently manufacturable cameras, what makes it particularly complex is that for humans, the meaning of a pixel value is not only defined by its surroundings and context, but also by what happened in the past, as shaped by the decades of learning experienced by the observer. 

As we begin to better understand our own visual system, it becomes clear that, just like humans, complex computer vision tasks must rely on prior knowledge to be performed successfully. This knowledge---or priors---must be extracted from collected experiences. Fortunately, the advent of social media has brought a phenomenal influx of images of all sorts each day to public databases, enabling the development of data-hungry machine learning algorithms such as deep neural networks. Such data-driven approaches are not only possible due to the newly available datasets, but also given the recent increase in computing speed and storage capacity of contemporary computers, allowing to handle the staggering amount of data publicly available nowadays. These machine learning methods bring a new paradigm to tackle vision problems: learning priors on natural images. These priors (initial beliefs on a probability distribution) are effectively additional constraints that can complement classical physics- or geometric-based approaches, which can improve solutions to ill-posed problems. 

This is the main topic of this dissertation: learning priors through machine learning to understand and solve problems that are ill-posed when considering them exclusively from a physics or geometric perspective. Specifically, three problems will be covered. First, the problem of 3D surface reconstruction using photometric cues, followed by outdoor lighting estimation and finally camera calibration. For all those problems, we are interested in cases when the environment is mostly uncontrolled, resulting in an under-constrained problem, impossible to solve robustly by classical approaches. 

% First chapter
First, we will focus on recovering the 3D surface of an object, which can be done by photometric stereo (PS). It is a popular dense shape reconstruction technique that has matured extensively over nearly 40 years~\cite{woodham-opteng-80}. % to work with complex materials and lighting conditions~\cite{alldrin-cvpr-08,basri-ijcv-07,johnson-cvpr-11,oxholm-eccv-12}.
%Given the excellent PS results obtained in carefully designed laboratory setups,
Simply put, this technique proposes to recover the surface normals of a 3D object observed under varying illumination from a single viewpoint. 
PS is reputed to give accurate surface normal estimation in fully calibrated environments, where lighting is controlled. 

Recent investigations have turned to the more challenging problem of outdoor PS under uncontrolled, natural illumination. To do so, people used the sun as a point light source to model outdoor illumination. However, the sun follows a mostly coplanar path throughout a single day, leading to photometric cues which does not create sufficient constraints to robustly solve the PS problem. Recent approaches proposed to capture images over the course of many months~\cite{ackermann-cvpr-12,abrams-eccv-12}. This time interval provides enough shifting to the solar plane to constrain correctly the PS problem under the point light source assumption, although waiting for this amount of time is tedious and impractical. 

In this thesis, we propose to leverage the richness of natural illumination to solve the outdoor photometric stereo problem in shorter time intervals. This brings us to our first main contribution: 

\begin{quotation}
\textbf{Short-Term Photometric Stereo} We present a systematic analysis of the expected performance of PS algorithms in outdoor settings on a single day or less, and propose to solve the short-term PS problem under various weather conditions by compensating the missing information from photometric cues with machine learned priors. 
\end{quotation}

By using a richer lighting model than the point light source to solve the outdoor PS problem, we are able to understand why and when outdoor photometric cues alone can result in a stable surface reconstruction. We further show that, in some cases, photometric cues alone cannot solve this problem robustly. In such cases, we propose to augment the photometric cues with learned priors in order to solve the PS problem in those hard cases. 

The second axis of research presented in this thesis explores the problem of estimating lighting from a single outdoor image. Outdoor lighting conditions throughout the day are mainly governed by the position of the sun and the quantity of aerosols like water vapor present in the atmosphere, which is called the turbidity. The goal of this research project is to estimate the sun position and the sky turbidity from an image of a generic scene taken from a standard camera. What makes this project particularly interesting is the variability of generic images, which may potentially contain the sky, or not. It may depict an urban scene, or be taken in various natural landscapes. This uncertainty about the image content makes the problem challenging. 

In this dissertation, we argue that machine learning can be used to learn priors on generic scenes and natural illumination. Using this additional information, it is possible to improve current single-image lighting estimation techniques, leading to our second contribution: 

\begin{quotation}
\textbf{Single Image Outdoor Lighting Estimation} We present a single-image learning-based approach to perform outdoor lighting estimation on generic scenes under natural daylight. 
\end{quotation}





The last research axis focuses on camera calibration from a single image of a generic scene. %This is a typically tedious process that typically requires the detection of specific image features in many images.
Usually, some specific object must be inserted in the scene, like a checkerboard pattern on a planar surface. Then, around a dozen images are taken, where this checkerboard is positioned in various locations and orientations in the image. Having to insert an object in the scene and capturing multiple images makes this process tedious and impossible to perform after-the-fact. To simplify it, one could perform their calibration using a single image of generic objects or features. However, this setup turns the calibration process into a severely ill-posed problem. 

In this thesis, we claim that it is possible to constrain the camera calibration problem to be robustly solved using a single image using learned priors on natural images, leading to our third contribution: 

\begin{quotation}
\textbf{Single Image Camera Calibration} We present a single-image learning-based approach to perform camera calibration. The proposed method works on generic scenes and illumination and do not require a specific object to be present in the image. 
\end{quotation}

Recent advances in machine learning allows learned models to encode the structure of large datasets, surpassing what would be possible with handcrafted feature extractors. However, the 

%Recent advances in machine learning allow for robust detection and classification of objects in images. By recognizing the objects present in a scene and comparing with their typical size in the physical world, one could estimate the focal length of the image. A similar process can be done with the object orientation and position, giving camera pitch and roll. Calibration would then be a matter of capturing common objects of the world and extracting their characteristics from the encoding of a machine learning model. 

The three contributions can have a direct impact on various applications. For instance, surface reconstruction can allow the preservation of historical landmarks or cultural heritage like statues and architectural features in high-definition. It is also useful for the entertainment industry, where scanning of real-life models is critical for realism in video games or the movie industry. Single-image lighting and camera calibration enables automated photorealistic virtual object insertion, image alteration and relighting. As such, some of the proposed methods have already been transfered to Dimension, the new 3D editing software from Adobe Systems. It could also be applied to forensics, allowing the analysis of potentially manipulated images. 

\section*{Overview}

This thesis is grouped into two main axes: 1) surface reconstruction through photometric cues, and 2) learning-based lighting and camera calibration estimation. All research axes are explored through a data-driven paradigm. 
% Concretely, each project leverages between thousands and millions of images to train models by optimizing several millions of parameters, executing trillions of floating-point operations within seconds to process data yielding hundreds of thousands of dimensions. It would be a euphemism to say that the presented research could not have been performed without the recent advances in both computing and storage.

First, chapter~\ref{ch1} provides an in-depth analysis of the information contained in photometric cues throughout a single day and gives performance bounds on Photometric Stereo when performed on intervals down to a single hour. Upon investigation, we found that partially cloudy days brought enough constraints to solve the PS problem using an adapted PS algorithm. However, we show that sunny days in general are lacking the constraints to provide a stable surface reconstruction. In chapter~\ref{ch2}, we go one step further than what is possible using exclusively photometric cues. Since there is not enough information in a single sunny day to do a stable surface reconstruction solely from photometric cues, we employ a deep learning model to learn priors on local surface geometry and sun trajectory patterns. This additional information brings enough supplemental constraints to the PS problem to allow stable surface reconstructions. 

Chapters~\ref{ch3} and~\ref{ch4} propose to extract priors from large datasets and utilize them to improve single-image outdoor lighting and camera calibration, respectively. Most current outdoor lighting estimation techniques rely exclusively on handcrafted features, limiting their application to a determined environment, for example urban scenes~\cite{lalonde-ijcv-12}. We propose a lighting estimation approach that is robust to generic scenes. We are able to do so by learning features on a large number of scenes that captures the essence of natural illumination, while devoid of specific content. Finally, the last chapter covers the problem of camera calibration from a single image. An emphasis is put on focal length estimation and extrinsic calibration with respect to the earth. To do so, we focus on finding the horizon within the image, even when it is hidden, like in most indoor scenes. We further analyze human tolerance to errors on those calibration parameters. 



%\bibliographystyle{abbrvnat}
%\bibliography{library}

